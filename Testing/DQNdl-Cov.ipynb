{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        #self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc1 = nn.Linear(in_features=linear_input_size, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=outputs)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        #t = F.relu(nn.BatchNorm2d(self.conv1(t)))\n",
    "        #t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #t = F.relu(nn.BatchNorm2d(self.conv2(t)))\n",
    "        #t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t = F.relu(self.bn1(self.conv1(t)))\n",
    "        t = F.relu(self.bn2(self.conv2(t)))\n",
    "        t = F.relu(self.bn3(self.conv3(t)))\n",
    "        \n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        #self.env = gym.make('CartPole-v1').unwrapped\n",
    "        #self.env = gym.make('Acrobot-v1').unwrapped\n",
    "        #self.env = gym.make('MountainCar-v0').unwrapped\n",
    "        self.env = gym.make('Breakout-v0').unwrapped\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "        \n",
    "    def take_action(self, action):        \n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects Channel*H*W\n",
    "        #screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[1]\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.4)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "   \n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "          moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9fcbaf29cd59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m#torch.save(policy_net.state_dict(), PATH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "em = EnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "\n",
    "init_screen = em.get_processed_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = em.num_actions_available()\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "try:\n",
    "    policy_net.load_state_dict(torch.load(PATH))\n",
    "    print('loaded successfully')\n",
    "except:\n",
    "    print('loading failed......')\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "params = policy_net.parameters()\n",
    "tb = SummaryWriter(comment=f'-{params}')\n",
    "\n",
    "#policy_net = DQN(em.get_screen_height, em.get_screen_width, em.num_actions_available).to(device)\n",
    "#target_net = DQN(em.get_screen_height, em.get_screen_width, em.num_actions_available).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "episode_durations = []\n",
    "episode_loss = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    total_loss = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        total_reward += reward\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            \n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            total_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #torch.save(policy_net.state_dict(), PATH)\n",
    "            #em.render()\n",
    "            \n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)\n",
    "            episode_loss.append(total_loss)\n",
    "            tb.add_scalar('Loss', total_loss, episode)\n",
    "            tb.add_scalar('Reward', total_reward, episode)\n",
    "            #print(episode_loss,'\\n')\n",
    "            plot(episode_durations, 100)\n",
    "            plt.plot(episode_loss)\n",
    "            break\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
